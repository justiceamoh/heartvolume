{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Neural Network Regressor - Rework for TBME\n",
    "\n",
    "My first stab at this task will be to implement an **DNN Regressor** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#Basic Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold01_log.csv\tFold04_log.csv\tFold07_log.csv\tFold10_log.csv\ttrain.csv\r\n",
      "Fold02_log.csv\tFold05_log.csv\tFold08_log.csv\tREADME.docx\r\n",
      "Fold03_log.csv\tFold06_log.csv\tFold09_log.csv\ttest.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls data7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data7/train.csv',header=None)\n",
    "# print df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_data = df.iloc[:,:-1].values.reshape((-1,30,224))\n",
    "# y_data = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load function\n",
    "def load_data(filename,prints=False,spatial=False):\n",
    "    data   = pd.read_csv(filename,header=None)\n",
    "    if spatial:\n",
    "        x_data = data.iloc[:,:-1].values.reshape((-1,30,224,1))\n",
    "    else:\n",
    "        x_data = data.iloc[:,:-1].values\n",
    "    y_data = data.iloc[:,-1].values\n",
    "    if prints:\n",
    "        print 'X shape:', x_data.shape\n",
    "        print 'Y shape:', y_data.shape\n",
    "    return x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataid   = 7\n",
    "filename = 'data{}/train.csv'.format(dataid)\n",
    "x_train,y_train=load_data(filename,spatial=True)\n",
    "scaler   = MinMaxScaler(feature_range=(0,1)).fit(y_train.reshape(-1,1))\n",
    "y_train  = scaler.transform(y_train.reshape(-1,1)).squeeze()\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(x_train,y_train,test_size=0.25,random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junior/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras Imports\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, AveragePooling2D, UpSampling2D\n",
    "from keras.layers import Flatten, Reshape,Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Params\n",
    "num_batch = 28\n",
    "num_epoch = 30\n",
    "patience  = 10\n",
    "\n",
    "# Parameters\n",
    "# ndim = x_train.shape[1]\n",
    "npat,ndim,_ = x_train.shape[1:]\n",
    "L1   = 127\n",
    "L2   = 169\n",
    "L3   = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build DNN\n",
    "# def build_model(cnn=False):\n",
    "#     if not cnn:\n",
    "#         x = Input(shape=(ndim,),name='Input')\n",
    "#         h = Dense(L1,activation='relu',name='L1')(x)\n",
    "#         h = BatchNormalization()(h)\n",
    "#         h = Dense(L2,activation='relu',kernel_regularizer=regularizers.l2(0.01),name='L2')(h)\n",
    "#         h = BatchNormalization()(h)\n",
    "#         h = Dense(L3,activation='relu',kernel_regularizer=regularizers.l2(0.01),name='L3')(h)\n",
    "#         h = BatchNormalization()(h)\n",
    "#         y = Dense(1,activation='linear',name='Output')(h)\n",
    "        \n",
    "#     else:\n",
    "#         x = Input(shape=(npat,ndim,1),name='Input')\n",
    "#         h = Conv2D(28,kernel_size=(3,5),strides=(2,3),padding='same',name='L1')(x)\n",
    "#         h = BatchNormalization()(h)\n",
    "#         h = Conv2D(16,kernel_size=(3,3),strides=(2,3),padding='same',name='L2')(h)\n",
    "#         h = BatchNormalization()(h)\n",
    "#         h = Conv2D(15,kernel_size=(3,3),strides=(1,3),padding='same',name='L3')(h)\n",
    "#         h = BatchNormalization()(h)\n",
    "#         h = AveragePooling2D((2,2),strides=(2,2))(h)\n",
    "#         h = Flatten()(h)\n",
    "#         h = Dense(7,activation='relu',kernel_regularizer=regularizers.l2(0.01),name='L4')(h)\n",
    "#         y = Dense(1,activation='linear',name='Output')(h)\n",
    "\n",
    "#     model = Model(x,y)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "x = Input(shape=(npat,ndim,1),name='Input')\n",
    "h = Conv2D(28,kernel_size=(3,5),strides=(2,3),padding='same',name='L1')(x)\n",
    "h = BatchNormalization()(h)\n",
    "h = Conv2D(16,kernel_size=(3,3),strides=(2,3),padding='same',name='L2')(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = Conv2D(15,kernel_size=(3,3),strides=(1,3),padding='same',name='L3')(h)\n",
    "h = BatchNormalization()(h)\n",
    "h = AveragePooling2D((2,2),strides=(2,2))(h)\n",
    "h = Flatten()(h)\n",
    "encoded = Dense(7,activation='relu',kernel_regularizer=regularizers.l2(0.01),name='L4')(h)\n",
    "\n",
    "# h = Flatten()(h)\n",
    "# encoded = Dense(7,activation='relu',kernel_regularizer=regularizers.l2(0.01),name='L4')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "y = Dense(525)(encoded)\n",
    "y = Reshape((5,7,15))(y)\n",
    "y = Conv2D(15,kernel_size=(3,3),padding='same')(y)\n",
    "y = UpSampling2D((3,4))(y)\n",
    "\n",
    "y = Conv2D(16,kernel_size=(3,3),padding='same')(y)\n",
    "y = UpSampling2D((2,4))(y)\n",
    "\n",
    "y = Conv2D(28,kernel_size=(3,5),padding='same')(y)\n",
    "y = UpSampling2D((1,2))(y)\n",
    "\n",
    "decoded = Conv2D(1,(3,3),activation='sigmoid',padding='same')(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x,decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 30, 224, 1)        0         \n",
      "_________________________________________________________________\n",
      "L1 (Conv2D)                  (None, 15, 75, 28)        448       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 15, 75, 28)        112       \n",
      "_________________________________________________________________\n",
      "L2 (Conv2D)                  (None, 8, 25, 16)         4048      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 25, 16)         64        \n",
      "_________________________________________________________________\n",
      "L3 (Conv2D)                  (None, 8, 9, 15)          2175      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 9, 15)          60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 4, 4, 15)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "L4 (Dense)                   (None, 7)                 1687      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 525)               4200      \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 5, 7, 15)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 7, 15)          2040      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 15, 28, 15)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 28, 16)        2176      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 30, 112, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 112, 28)       6748      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 30, 224, 28)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 224, 1)        253       \n",
      "=================================================================\n",
      "Total params: 24,011\n",
      "Trainable params: 23,893\n",
      "Non-trainable params: 118\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname  = 'autoencoder.h5'\n",
    "# e_stopper  = EarlyStopping(monitor='val_loss', min_delta=0, patience=10)\n",
    "# checkpoint = ModelCheckpoint(modelname,monitor='val_loss',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "log = model.fit(x_train, x_train,\n",
    "              batch_size=num_batch,\n",
    "              epochs=num_epoch,\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "model = build_model(cnn=True)\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "\n",
    "modelname  = 'hypermodel.h5'\n",
    "e_stopper  = EarlyStopping(monitor='val_loss', min_delta=0, patience=10)\n",
    "checkpoint = ModelCheckpoint(modelname,monitor='val_loss',verbose=1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "log = model.fit(x_train, y_train,\n",
    "              batch_size=num_batch,\n",
    "              epochs=num_epoch,\n",
    "              shuffle=True,\n",
    "              callbacks=[e_stopper,checkpoint],\n",
    "              validation_data=(x_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model{}.h5'.format(dataid))\n",
    "model = load_model(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training Curves\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(log.epoch,log.history['loss'])\n",
    "plt.plot(log.epoch,log.history['val_loss'],'g')\n",
    "plt.title('Training Curves')\n",
    "# plt.ylim([0,0.05])\n",
    "plt.xlabel('Epochs')\n",
    "plt.xlabel('MSE Loss')\n",
    "plt.legend(['Train','Valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate Predictions for Test Set\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "fname = 'data{}/test.csv'.format(dataid)\n",
    "x_test,y_test=load_data(fname,spatial=True)\n",
    "y_test = scaler.transform(y_test.reshape(-1,1)).squeeze()\n",
    "y_predict = model.predict(x_test)\n",
    "loss = model.evaluate(x_test,y_test,verbose=0)\n",
    "\n",
    "\n",
    "# y_tst = scaler.inverse_transform(y_test.squeeze())\n",
    "# y_prd = scaler.inverse_transform(y_predict.squeeze())\n",
    "plt.subplot(111)\n",
    "plt.plot(y_test,'o-',alpha=0.5)\n",
    "plt.plot(y_predict,'go-')\n",
    "header = '{0}, mse={1:.02f}'.format(fname[9:-4],loss)\n",
    "plt.title(header)\n",
    "\n",
    "# oname = 'data{}/ypred.csv'.format(dataid)\n",
    "# tname = 'data{}/ytest.csv'.format(dataid)\n",
    "# np.savetxt(oname,y_predict,delimiter=',')\n",
    "# np.savetxt(tname,y_test,delimiter=',')\n",
    "\n",
    "plt.legend(['Original','Predicted'],loc=1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
